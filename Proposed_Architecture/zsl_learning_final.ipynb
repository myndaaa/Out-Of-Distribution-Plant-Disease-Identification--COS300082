{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2276482-5021-4a02-8314-bb9745089d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Import TensorFlow and Keras modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import scikit-learn utilities\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Keras backend\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output (optional)\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc2ff9-dafa-45f2-8d4a-73c3e82e9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where datasets and models are stored\n",
    "base_dir = r'C:\\Users\\bheja\\OneDrive\\Desktop\\Dataset'\n",
    "\n",
    "# Define directories for PlantVillage and PlantDoc datasets\n",
    "plant_vil_dir = os.path.join(base_dir, 'plantvillage')\n",
    "plant_doc_dir = os.path.join(base_dir, 'plantdoc')\n",
    "\n",
    "# Define paths for CSV files\n",
    "train_csv_path = os.path.join(base_dir, 'PV_train.csv')\n",
    "test_seen_csv_path = os.path.join(base_dir, 'PV_test_seen.csv')\n",
    "test_unseen_csv_path = os.path.join(base_dir, 'PV_test_unseen.csv')\n",
    "doc_unseen_csv_path = os.path.join(base_dir, 'PD_test_unseen.csv')\n",
    "\n",
    "# Define path for the trained ResNet50V2 model\n",
    "trained_model_path = r'C:\\Users\\bheja\\OneDrive\\Desktop\\models\\resnet50v2_crop.h5'\n",
    "\n",
    "# Load CSV files into pandas DataFrames\n",
    "train_data = pd.read_csv(train_csv_path, header=None, names=[\"image_name\", \"crop_class\", \"disease_class\"])\n",
    "test_seen_data = pd.read_csv(test_seen_csv_path, header=None, names=[\"image_name\", \"crop_class\", \"disease_class\"])\n",
    "test_unseen_data = pd.read_csv(test_unseen_csv_path, header=None, names=[\"image_name\", \"crop_class\", \"disease_class\"])\n",
    "doc_unseen_data = pd.read_csv(doc_unseen_csv_path, header=None, names=[\"image_name\", \"crop_class\", \"disease_class\"])\n",
    "\n",
    "# Display the first few rows of the training data to verify\n",
    "print(\"Sample Training Data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Verify the data types of the relevant columns\n",
    "print(\"\\nData Types:\")\n",
    "print(train_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a597-aa41-43b5-af30-e60f0feddd8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the trained ResNet50V2 model\n",
    "try:\n",
    "    model = load_model(trained_model_path, compile=False)\n",
    "    print(\"ResNet50V2 model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7d14a-7282-466e-acc2-c9ce01d89127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all layers with their indices and names\n",
    "for idx, layer in enumerate(model.layers):\n",
    "    print(f\"{idx}: {layer.name} - {layer.output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6491a-22ce-4451-99ef-a8c9633690de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd4b00-11c9-4d47-80f5-34569159971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Specify the name of the target layer for feature extraction\n",
    "target_layer_name = 'global_average_pooling2d_2'\n",
    "\n",
    "# Retrieve the output of the target layer\n",
    "try:\n",
    "    target_layer_output = model.get_layer(name=target_layer_name).output\n",
    "    print(f\"Successfully retrieved the output of layer: {target_layer_name}\")\n",
    "except ValueError:\n",
    "    print(f\"Layer {target_layer_name} not found. Please check the layer name.\")\n",
    "    # Optionally, list all layer names for reference\n",
    "    for layer in model.layers:\n",
    "        print(layer.name)\n",
    "    raise\n",
    "\n",
    "# Create the Feature Extractor Model\n",
    "feature_extractor = Model(inputs=model.input, outputs=target_layer_output)\n",
    "\n",
    "print(\"Feature extractor model created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ba606-ada3-4cd9-a59d-666fe2e80a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define ImageDataGenerator for feature extraction (no augmentation needed)\n",
    "feature_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def create_feature_generator_optimized(dataframe, directory, target_size=(224, 224), batch_size=128, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates an optimized ImageDataGenerator for feature extraction with multiple workers.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame containing image paths.\n",
    "        directory (str): Directory where images are stored.\n",
    "        target_size (tuple): Desired image size.\n",
    "        batch_size (int): Number of images per batch.\n",
    "        shuffle (bool): Whether to shuffle the data.\n",
    "    \n",
    "    Returns:\n",
    "        Iterator: Keras generator yielding batches of images.\n",
    "    \"\"\"\n",
    "    generator = feature_datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=directory,\n",
    "        x_col=\"image_name\",\n",
    "        y_col=None,  # No labels needed for feature extraction\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=shuffle,\n",
    "        seed=42,\n",
    "        workers=4,  # Number of parallel workers (adjust based on your CPU)\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "# Recreate the optimized generators with a larger batch size\n",
    "train_feature_gen_optimized = create_feature_generator_optimized(\n",
    "    dataframe=train_data,\n",
    "    directory=plant_vil_dir,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_seen_feature_gen_optimized = create_feature_generator_optimized(\n",
    "    dataframe=test_seen_data,\n",
    "    directory=plant_vil_dir,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_unseen_feature_gen_optimized = create_feature_generator_optimized(\n",
    "    dataframe=test_unseen_data,\n",
    "    directory=plant_vil_dir,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "doc_unseen_feature_gen_optimized = create_feature_generator_optimized(\n",
    "    dataframe=doc_unseen_data,\n",
    "    directory=plant_doc_dir,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Optimized feature data generators created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44770f-2010-4df2-924c-1a8053295142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_features_direct(generator, model, num_samples):\n",
    "    \"\"\"\n",
    "    Extracts features from all images in a generator using the provided model.\n",
    "    Utilizes Keras's built-in predict method for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        generator (Iterator): Keras ImageDataGenerator iterator.\n",
    "        model (tf.keras.Model): Feature extractor model.\n",
    "        num_samples (int): Total number of samples to process.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Extracted features.\n",
    "    \"\"\"\n",
    "    # Calculate the number of steps (batches)\n",
    "    steps = int(np.ceil(num_samples / generator.batch_size))\n",
    "    \n",
    "    # Use Keras's predict method with generator\n",
    "    features = model.predict(generator, steps=steps, verbose=1)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e14b5-0cbf-44cc-acd7-6af19f004c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples in each dataset\n",
    "num_train = train_feature_gen_optimized.n\n",
    "num_test_seen = test_seen_feature_gen_optimized.n\n",
    "num_test_unseen = test_unseen_feature_gen_optimized.n\n",
    "num_doc_unseen = doc_unseen_feature_gen_optimized.n\n",
    "\n",
    "# Extract features for training data\n",
    "print(\"Extracting Training Features...\")\n",
    "train_features = extract_features_direct(\n",
    "    generator=train_feature_gen_optimized,\n",
    "    model=feature_extractor,\n",
    "    num_samples=num_train\n",
    ")\n",
    "print(f\"Training features extracted: {train_features.shape}\")\n",
    "\n",
    "# Extract features for seen test data\n",
    "print(\"\\nExtracting Seen Test Features...\")\n",
    "test_seen_features = extract_features_direct(\n",
    "    generator=test_seen_feature_gen_optimized,\n",
    "    model=feature_extractor,\n",
    "    num_samples=num_test_seen\n",
    ")\n",
    "print(f\"Seen Test features extracted: {test_seen_features.shape}\")\n",
    "\n",
    "# Extract features for unseen test data\n",
    "print(\"\\nExtracting Unseen Test Features...\")\n",
    "test_unseen_features = extract_features_direct(\n",
    "    generator=test_unseen_feature_gen_optimized,\n",
    "    model=feature_extractor,\n",
    "    num_samples=num_test_unseen\n",
    ")\n",
    "print(f\"Unseen Test features extracted: {test_unseen_features.shape}\")\n",
    "\n",
    "# Extract features for PlantDoc unseen test data\n",
    "print(\"\\nExtracting PlantDoc Unseen Test Features...\")\n",
    "doc_unseen_features = extract_features_direct(\n",
    "    generator=doc_unseen_feature_gen_optimized,\n",
    "    model=feature_extractor,\n",
    "    num_samples=num_doc_unseen\n",
    ")\n",
    "print(f\"PlantDoc Unseen Test features extracted: {doc_unseen_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84790549-6dbe-4791-a2b8-6696568c4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define directory to save features\n",
    "features_dir = r'C:\\Users\\bheja\\OneDrive\\Desktop\\features'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "# Save features as .npy files\n",
    "np.save(os.path.join(features_dir, 'train_features.npy'), train_features)\n",
    "np.save(os.path.join(features_dir, 'test_seen_features.npy'), test_seen_features)\n",
    "np.save(os.path.join(features_dir, 'test_unseen_features.npy'), test_unseen_features)\n",
    "np.save(os.path.join(features_dir, 'doc_unseen_features.npy'), doc_unseen_features)\n",
    "\n",
    "print(\"All features saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476257d-b352-4f92-a228-700988b4c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define directory where features are saved\n",
    "features_dir = r'C:\\Users\\bheja\\OneDrive\\Desktop\\features'\n",
    "\n",
    "# Load features from .npy files\n",
    "train_features = np.load(os.path.join(features_dir, 'train_features.npy'))\n",
    "test_seen_features = np.load(os.path.join(features_dir, 'test_seen_features.npy'))\n",
    "test_unseen_features = np.load(os.path.join(features_dir, 'test_unseen_features.npy'))\n",
    "doc_unseen_features = np.load(os.path.join(features_dir, 'doc_unseen_features.npy'))\n",
    "\n",
    "print(\"All features loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b88c0b-f09d-4056-9ae8-e5d4c522d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "print(f\"Training Features Shape: {train_features.shape}\")\n",
    "print(f\"Seen Test Features Shape: {test_seen_features.shape}\")\n",
    "print(f\"Unseen Test Features Shape: {test_unseen_features.shape}\")\n",
    "print(f\"PlantDoc Unseen Test Features Shape: {doc_unseen_features.shape}\")\n",
    "\n",
    "# Display a sample feature vector\n",
    "print(\"\\nSample Training Feature Vector (first 5 elements):\")\n",
    "print(train_features[0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e72378-c59e-475e-a0d8-b2077da83d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'crop_class' labels\n",
    "train_data['encoded_crop_class'] = label_encoder.fit_transform(train_data['crop_class'])\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"\\nEncoded Labels:\")\n",
    "print(train_data[['crop_class', 'encoded_crop_class']].head())\n",
    "print(\"\\nUnique Encoded Classes:\", train_data['encoded_crop_class'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb01cab-9aa9-4be0-bf60-09fce0b250e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode the 'crop_class' labels in validation data using the same LabelEncoder\n",
    "test_seen_data['encoded_crop_class'] = label_encoder.transform(test_seen_data['crop_class'])\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"\\nEncoded Validation Labels:\")\n",
    "print(test_seen_data[['crop_class', 'encoded_crop_class']].head())\n",
    "\n",
    "# Display unique encoded classes in validation data\n",
    "print(\"\\nUnique Encoded Classes in Validation Data:\", test_seen_data['encoded_crop_class'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7aab2-bb8b-4684-b468-8f442e6d7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import random\n",
    "\n",
    "def create_pairs(features, labels, num_pairs):\n",
    "    \"\"\"\n",
    "    Create positive and negative pairs of feature vectors.\n",
    "\n",
    "    Args:\n",
    "        features (np.array): Feature vectors.\n",
    "        labels (np.array): Corresponding labels.\n",
    "        num_pairs (int): Number of pairs to create.\n",
    "\n",
    "    Returns:\n",
    "        pairs (np.array): Array of paired feature vectors.\n",
    "        pair_labels (np.array): Array of labels (1 for similar, 0 for dissimilar).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    # Create a dictionary mapping labels to indices\n",
    "    label_to_indices = {label: np.where(labels == label)[0] for label in np.unique(labels)}\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        # Decide whether to create a positive or negative pair\n",
    "        if random.random() < 0.5:\n",
    "            # Positive pair\n",
    "            label = random.choice(list(label_to_indices.keys()))\n",
    "            idx1, idx2 = np.random.choice(label_to_indices[label], 2, replace=False)\n",
    "            pairs.append([features[idx1], features[idx2]])\n",
    "            pair_labels.append(1)\n",
    "        else:\n",
    "            # Negative pair\n",
    "            label1, label2 = random.sample(list(label_to_indices.keys()), 2)\n",
    "            idx1 = random.choice(label_to_indices[label1])\n",
    "            idx2 = random.choice(label_to_indices[label2])\n",
    "            pairs.append([features[idx1], features[idx2]])\n",
    "            pair_labels.append(0)\n",
    "    \n",
    "    return np.array(pairs), np.array(pair_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e0227-777f-433f-9c5a-acb92c8fc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of training pairs you want to create\n",
    "num_train_pairs = 50000  # Adjust based on your dataset size and requirements\n",
    "\n",
    "# Extract training labels as a NumPy array\n",
    "train_labels_array = train_data['encoded_crop_class'].values\n",
    "\n",
    "# Create training pairs using the provided create_pairs function\n",
    "train_pair_features, train_pair_labels = create_pairs(train_features, train_labels_array, num_train_pairs)\n",
    "\n",
    "# Check the number of created pairs\n",
    "print(f\"Total Training Pairs Created: {len(train_pair_features)}\")\n",
    "print(f\"Total Training Labels Created: {len(train_pair_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d85530-a1ef-405e-8c39-4db7999fe483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of validation pairs you want to create\n",
    "num_val_pairs = 10000  # Adjust based on your dataset size and requirements\n",
    "\n",
    "# Extract validation labels as a NumPy array\n",
    "val_labels_array = test_seen_data['encoded_crop_class'].values\n",
    "\n",
    "# Create validation pairs using the provided create_pairs function\n",
    "val_pair_features, val_pair_labels = create_pairs(test_seen_features, val_labels_array, num_val_pairs)\n",
    "\n",
    "# Check the number of created pairs\n",
    "print(f\"Total Validation Pairs Created: {len(val_pair_features)}\")\n",
    "print(f\"Total Validation Labels Created: {len(val_pair_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd761ccf-be97-40b0-9540-d9e8f9ee09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training pairs into two arrays\n",
    "train_pair_features_1 = np.array([pair[0] for pair in train_pair_features])\n",
    "train_pair_features_2 = np.array([pair[1] for pair in train_pair_features])\n",
    "train_pair_labels = np.array(train_pair_labels)\n",
    "\n",
    "# Display the shapes to verify\n",
    "print(f\"Training Pair Features 1 Shape: {train_pair_features_1.shape}\")\n",
    "print(f\"Training Pair Features 2 Shape: {train_pair_features_2.shape}\")\n",
    "print(f\"Training Pair Labels Shape: {train_pair_labels.shape}\")\n",
    "\n",
    "# Separate the validation pairs into two arrays\n",
    "val_pair_features_1 = np.array([pair[0] for pair in val_pair_features])\n",
    "val_pair_features_2 = np.array([pair[1] for pair in val_pair_features])\n",
    "val_pair_labels = np.array(val_pair_labels)\n",
    "\n",
    "# Display the shapes to verify\n",
    "print(f\"Validation Pair Features 1 Shape: {val_pair_features_1.shape}\")\n",
    "print(f\"Validation Pair Features 2 Shape: {val_pair_features_2.shape}\")\n",
    "print(f\"Validation Pair Labels Shape: {val_pair_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60f7dd-3789-448b-a420-aa08bb2c79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    \"\"\"\n",
    "    Creates the base network for feature processing.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input feature vector.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Base network model.\n",
    "    \"\"\"\n",
    "    input = Input(shape=(input_dim,), name='Base_Input')\n",
    "    x = Dense(512, activation='relu', name='Dense_512')(input)\n",
    "    x = Dense(256, activation='relu', name='Dense_256')(x)\n",
    "    x = Dense(128, activation='relu', name='Dense_128')(x)\n",
    "    return Model(inputs=input, outputs=x, name='Base_Network')\n",
    "\n",
    "# Define input dimension\n",
    "input_dim = train_features.shape[1]  # 2048\n",
    "\n",
    "# Create the base network\n",
    "base_network = create_base_network(input_dim)\n",
    "\n",
    "# Display the base network summary\n",
    "base_network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c7c2c-6a84-46dc-a9d0-5bc212567c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vects (list): List containing two tensors.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Euclidean distance.\n",
    "    \"\"\"\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def create_siamese_network(base_network):\n",
    "    \"\"\"\n",
    "    Creates a Siamese Network using the provided base network.\n",
    "\n",
    "    Args:\n",
    "        base_network (keras.Model): Base network to process input features.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Siamese Network model.\n",
    "    \"\"\"\n",
    "    input_a = Input(shape=(base_network.input_shape[1],), name='Input_A')\n",
    "    input_b = Input(shape=(base_network.input_shape[1],), name='Input_B')\n",
    "    \n",
    "    # Process both inputs through the base network\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    # Compute the Euclidean distance between the two processed inputs\n",
    "    distance = Lambda(euclidean_distance, name='Euclidean_Distance')([processed_a, processed_b])\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    output = Dense(1, activation='sigmoid', name='Similarity')(distance)\n",
    "    \n",
    "    # Define the Siamese Network model\n",
    "    siamese_network = Model(inputs=[input_a, input_b], outputs=output, name='Siamese_Network')\n",
    "    \n",
    "    return siamese_network\n",
    "\n",
    "# Create the Siamese Network\n",
    "siamese_network = create_siamese_network(base_network)\n",
    "\n",
    "# Display the Siamese Network summary\n",
    "siamese_network.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d289b-8914-403e-9668-5432bfa5c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "siamese_network.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Siamese Network compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438db904-779e-452b-b9ba-0cee47110ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Define the path to save the best model\n",
    "checkpoint_path = os.path.join(features_dir, 'siamese_network_best.h5')\n",
    "\n",
    "# EarlyStopping callback to stop training when validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation loss\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Combine all callbacks into a list\n",
    "callbacks = [early_stopping, reduce_lr, model_checkpoint]\n",
    "\n",
    "print(\"Callbacks defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81899639-9229-4134-922d-39006cd295a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9a999-0be3-47c4-8098-ed7057c0b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs and batch size\n",
    "epochs = 20  # You can adjust this based on your requirements\n",
    "batch_size = 128  # Adjust based on your system's memory capacity\n",
    "\n",
    "# Train the Siamese Network\n",
    "history = siamese_network.fit(\n",
    "    [train_pair_features_1, train_pair_features_2],\n",
    "    train_pair_labels,\n",
    "    validation_data=([val_pair_features_1, val_pair_features_2], val_pair_labels),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1  # Set to 1 to see progress bar, 2 for one line per epoch\n",
    ")\n",
    "\n",
    "print(\"Training completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763538d-1f59-4275-97b7-cd7ffd32d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Define the path to the best saved model\n",
    "best_model_path = os.path.join(features_dir, 'siamese_network_best.h5')\n",
    "\n",
    "# Check if the best model file exists\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Best model found at: {best_model_path}\")\n",
    "else:\n",
    "    print(f\"Best model not found at: {best_model_path}. Please ensure the model was saved correctly.\")\n",
    "\n",
    "# Load the best saved Siamese Network model\n",
    "try:\n",
    "    best_siamese_model = load_model(best_model_path, compile=False)\n",
    "    print(\"Best Siamese Network model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the best Siamese Network model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebb9b7-49e8-4b75-ba3a-41b9494b146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_class_prototypes(features, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Creates prototype feature vectors for each class.\n",
    "\n",
    "    Args:\n",
    "        features (np.array): Array of feature vectors.\n",
    "        labels (np.array): Array of encoded class labels.\n",
    "        num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        prototypes (dict): Dictionary mapping class labels to prototype vectors.\n",
    "    \"\"\"\n",
    "    prototypes = {}\n",
    "    for cls in range(num_classes):\n",
    "        class_features = features[labels == cls]\n",
    "        if len(class_features) == 0:\n",
    "            print(f\"Warning: No samples for class {cls}\")\n",
    "            continue\n",
    "        prototype = np.mean(class_features, axis=0)\n",
    "        prototypes[cls] = prototype\n",
    "    return prototypes\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Create prototypes for seen classes using training data\n",
    "train_prototypes = create_class_prototypes(train_features, train_labels_array, num_classes)\n",
    "\n",
    "# Display a prototype for the first class\n",
    "first_class = label_encoder.inverse_transform([0])[0]\n",
    "print(f\"Prototype for class '{first_class}': {train_prototypes[0][:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3bd75-159b-480d-8d5c-fd35968ae94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def classify_image(feature, prototypes, top_k=5):\n",
    "    \"\"\"\n",
    "    Classifies an image feature by finding the top_k closest class prototypes based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        feature (np.array): Feature vector of the image.\n",
    "        prototypes (dict): Dictionary of class prototypes.\n",
    "        top_k (int): Number of top similar classes to consider.\n",
    "\n",
    "    Returns:\n",
    "        top_classes (list): List of top_k predicted class labels.\n",
    "        similarity_scores (list): List of corresponding similarity scores.\n",
    "    \"\"\"\n",
    "    similarity_scores = {}\n",
    "    for cls, prototype in prototypes.items():\n",
    "        similarity = 1 - cosine(feature, prototype)  # Cosine similarity\n",
    "        similarity_scores[cls] = similarity\n",
    "\n",
    "    # Sort classes based on similarity scores in descending order\n",
    "    sorted_classes = sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Extract top_k classes and their scores\n",
    "    top_classes = [label_encoder.inverse_transform([cls])[0] for cls, score in sorted_classes[:top_k]]\n",
    "    similarity_scores_sorted = [score for cls, score in sorted_classes[:top_k]]\n",
    "    \n",
    "    return top_classes, similarity_scores_sorted\n",
    "\n",
    "def evaluate_model_on_dataset(features, labels, prototypes, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.array): Array of feature vectors.\n",
    "        labels (np.array): Array of true encoded class labels.\n",
    "        prototypes (dict): Dictionary of class prototypes.\n",
    "        top_k (int): Number of top similar classes to consider.\n",
    "\n",
    "    Returns:\n",
    "        top1_accuracy (float): Top-1 accuracy.\n",
    "        top5_accuracy (float): Top-5 accuracy.\n",
    "    \"\"\"\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = len(labels)\n",
    "    \n",
    "    for i in range(total):\n",
    "        feature = features[i]\n",
    "        true_label = label_encoder.inverse_transform([labels[i]])[0]\n",
    "        predicted_topk, _ = classify_image(feature, prototypes, top_k=top_k)\n",
    "        \n",
    "        if true_label == predicted_topk[0]:\n",
    "            correct_top1 += 1\n",
    "            correct_top5 += 1\n",
    "        elif true_label in predicted_topk:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    top1_accuracy = correct_top1 / total\n",
    "    top5_accuracy = correct_top5 / total\n",
    "    \n",
    "    return top1_accuracy, top5_accuracy\n",
    "\n",
    "print(\"Inference functions defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599e7d4-adea-42da-a1c7-3d36f367d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training labels as a NumPy array\n",
    "train_labels_eval = train_data['encoded_crop_class'].values\n",
    "\n",
    "# Evaluate on Training Data\n",
    "train_top1_acc, train_top5_acc = evaluate_model_on_dataset(\n",
    "    features=train_features,\n",
    "    labels=train_labels_eval,\n",
    "    prototypes=train_prototypes,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Training Data - Top-1 Accuracy: {train_top1_acc * 100:.2f}%\")\n",
    "print(f\"Training Data - Top-5 Accuracy: {train_top5_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7aa48e-9d05-4984-879d-83f395fbdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract validation labels as a NumPy array\n",
    "val_labels_eval = test_seen_data['encoded_crop_class'].values\n",
    "\n",
    "# Evaluate on Test Seen Data (Validation Set)\n",
    "val_top1_acc, val_top5_acc = evaluate_model_on_dataset(\n",
    "    features=test_seen_features,\n",
    "    labels=val_labels_eval,\n",
    "    prototypes=train_prototypes,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Test Seen Data (Validation) - Top-1 Accuracy: {val_top1_acc * 100:.2f}%\")\n",
    "print(f\"Test Seen Data (Validation) - Top-5 Accuracy: {val_top5_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9503d0-1a00-4c52-87e9-ae25005ed9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'crop_class' labels in test_unseen_data using the same LabelEncoder\n",
    "try:\n",
    "    test_unseen_data['encoded_crop_class'] = label_encoder.transform(test_unseen_data['crop_class'])\n",
    "    print(\"\\nEncoded Test Unseen Labels:\")\n",
    "    print(test_unseen_data[['crop_class', 'encoded_crop_class']].head())\n",
    "    \n",
    "    # Display unique encoded classes in test_unseen_data\n",
    "    print(\"\\nUnique Encoded Classes in Test Unseen Data:\", test_unseen_data['encoded_crop_class'].unique())\n",
    "except ValueError as e:\n",
    "    print(f\"Error encoding test_unseen_data: {e}\")\n",
    "    \n",
    "    # Identify classes not in label_encoder\n",
    "    unseen_classes = set(test_unseen_data['crop_class']) - set(label_encoder.classes_)\n",
    "    print(f\"Unseen classes in test_unseen_data: {unseen_classes}\")\n",
    "    \n",
    "    # Optionally, you can handle unseen classes here\n",
    "    # For example, you can assign a special label or exclude these samples\n",
    "    # Here's how to assign a special label (e.g., -1) to unseen classes:\n",
    "    test_unseen_data['encoded_crop_class'] = test_unseen_data['crop_class'].apply(\n",
    "        lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1\n",
    "    )\n",
    "    print(\"\\nAfter handling unseen classes:\")\n",
    "    print(test_unseen_data[['crop_class', 'encoded_crop_class']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2050c-34b2-43c1-a62b-4818a2a39480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test unseen labels as a NumPy array\n",
    "test_unseen_labels_eval = test_unseen_data['encoded_crop_class'].values\n",
    "\n",
    "# Evaluate on Test Unseen Data\n",
    "test_unseen_top1_acc, test_unseen_top5_acc = evaluate_model_on_dataset(\n",
    "    features=test_unseen_features,\n",
    "    labels=test_unseen_labels_eval,\n",
    "    prototypes=train_prototypes,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Test Unseen Data - Top-1 Accuracy: {test_unseen_top1_acc * 100:.2f}%\")\n",
    "print(f\"Test Unseen Data - Top-5 Accuracy: {test_unseen_top5_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e9f2a1-429b-4353-a4c4-35c2ce947051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'crop_class' labels in doc_unseen_data using the same LabelEncoder\n",
    "try:\n",
    "    doc_unseen_data['encoded_crop_class'] = label_encoder.transform(doc_unseen_data['crop_class'])\n",
    "    print(\"\\nEncoded PlantDoc Unseen Labels:\")\n",
    "    print(doc_unseen_data[['crop_class', 'encoded_crop_class']].head())\n",
    "    \n",
    "    # Display unique encoded classes in doc_unseen_data\n",
    "    print(\"\\nUnique Encoded Classes in PlantDoc Unseen Data:\", doc_unseen_data['encoded_crop_class'].unique())\n",
    "except ValueError as e:\n",
    "    print(f\"Error encoding doc_unseen_data: {e}\")\n",
    "    \n",
    "    # Identify classes not in label_encoder\n",
    "    unseen_classes = set(doc_unseen_data['crop_class']) - set(label_encoder.classes_)\n",
    "    print(f\"Unseen classes in doc_unseen_data: {unseen_classes}\")\n",
    "    \n",
    "    # Optionally, handle unseen classes similarly\n",
    "    doc_unseen_data['encoded_crop_class'] = doc_unseen_data['crop_class'].apply(\n",
    "        lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1\n",
    "    )\n",
    "    print(\"\\nAfter handling unseen classes:\")\n",
    "    print(doc_unseen_data[['crop_class', 'encoded_crop_class']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900a259-6d62-4c42-b1ce-6fd3addc543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PlantDoc unseen labels as a NumPy array\n",
    "doc_unseen_labels_eval = doc_unseen_data['encoded_crop_class'].values\n",
    "\n",
    "# Evaluate on PlantDoc Unseen Test Data\n",
    "doc_unseen_top1_acc, doc_unseen_top5_acc = evaluate_model_on_dataset(\n",
    "    features=doc_unseen_features,\n",
    "    labels=doc_unseen_labels_eval,\n",
    "    prototypes=train_prototypes,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"PlantDoc Unseen Test Data - Top-1 Accuracy: {doc_unseen_top1_acc * 100:.2f}%\")\n",
    "print(f\"PlantDoc Unseen Test Data - Top-5 Accuracy: {doc_unseen_top5_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea48b34-05a1-4b33-873e-19084a553a01",
   "metadata": {},
   "source": [
    "DISEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8cb2c-1e11-4b88-8f26-11a73bd0476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder for disease_class\n",
    "disease_label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'disease_class' labels for training data\n",
    "train_data['encoded_disease_class'] = disease_label_encoder.fit_transform(train_data['disease_class'])\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"\\nEncoded Disease Labels (Training Data):\")\n",
    "print(train_data[['disease_class', 'encoded_disease_class']].head())\n",
    "\n",
    "print(\"\\nUnique Encoded Disease Classes:\", train_data['encoded_disease_class'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07348d54-a862-41fe-9853-2b8e172cabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# number of disease training and validation paris\n",
    "num_train_disease_pairs = 50000 \n",
    "num_val_disease_pairs = 10000    \n",
    "\n",
    "# Extract disease labels as a NumPy array\n",
    "train_disease_labels_array = train_data['encoded_disease_class'].values\n",
    "val_disease_labels_array = test_seen_data['encoded_disease_class'].values\n",
    "\n",
    "# Create disease training pairs\n",
    "train_disease_pair_features, train_disease_pair_labels = create_pairs(\n",
    "    features=train_features,\n",
    "    labels=train_disease_labels_array,\n",
    "    num_pairs=num_train_disease_pairs\n",
    ")\n",
    "\n",
    "print(f\"Total Disease Training Pairs Created: {len(train_disease_pair_features)}\")\n",
    "print(f\"Total Disease Training Labels Created: {len(train_disease_pair_labels)}\")\n",
    "\n",
    "# Create disease validation pairs\n",
    "val_disease_pair_features, val_disease_pair_labels = create_pairs(\n",
    "    features=test_seen_features,\n",
    "    labels=val_disease_labels_array,\n",
    "    num_pairs=num_val_disease_pairs\n",
    ")\n",
    "\n",
    "print(f\"Total Disease Validation Pairs Created: {len(val_disease_pair_features)}\")\n",
    "print(f\"Total Disease Validation Labels Created: {len(val_disease_pair_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b46f097-20c6-497b-ae29-c536b1151c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the disease training pairs into two arrays\n",
    "train_disease_pair_features_1 = np.array([pair[0] for pair in train_disease_pair_features])\n",
    "train_disease_pair_features_2 = np.array([pair[1] for pair in train_disease_pair_features])\n",
    "train_disease_pair_labels = np.array(train_disease_pair_labels)\n",
    "\n",
    "# Display the shapes to verify\n",
    "print(f\"Disease Training Pair Features 1 Shape: {train_disease_pair_features_1.shape}\")\n",
    "print(f\"Disease Training Pair Features 2 Shape: {train_disease_pair_features_2.shape}\")\n",
    "print(f\"Disease Training Pair Labels Shape: {train_disease_pair_labels.shape}\")\n",
    "\n",
    "# Separate the disease validation pairs into two arrays\n",
    "val_disease_pair_features_1 = np.array([pair[0] for pair in val_disease_pair_features])\n",
    "val_disease_pair_features_2 = np.array([pair[1] for pair in val_disease_pair_features])\n",
    "val_disease_pair_labels = np.array(val_disease_pair_labels)\n",
    "\n",
    "# Display the shapes to verify\n",
    "print(f\"Disease Validation Pair Features 1 Shape: {val_disease_pair_features_1.shape}\")\n",
    "print(f\"Disease Validation Pair Features 2 Shape: {val_disease_pair_features_2.shape}\")\n",
    "print(f\"Disease Validation Pair Labels Shape: {val_disease_pair_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9dcbb-09a5-4aa8-b896-ef43007b8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Lambda, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vects (list): List containing two tensors.\n",
    "\n",
    "    Returns:\n",
    "        tensor: Euclidean distance.\n",
    "    \"\"\"\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def create_siamese_network_shared_base(base_network):\n",
    "    \"\"\"\n",
    "    Creates a Siamese Network using the provided base network.\n",
    "\n",
    "    Args:\n",
    "        base_network (keras.Model): Base network to process input features.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Siamese Network model.\n",
    "    \"\"\"\n",
    "    input_a = Input(shape=(base_network.input_shape[1],), name='Disease_Input_A')\n",
    "    input_b = Input(shape=(base_network.input_shape[1],), name='Disease_Input_B')\n",
    "    \n",
    "    # Process both inputs through the base network\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    # Compute the Euclidean distance between the two processed inputs\n",
    "    distance = Lambda(euclidean_distance, name='Disease_Euclidean_Distance')([processed_a, processed_b])\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    output = Dense(1, activation='sigmoid', name='Disease_Similarity')(distance)\n",
    "    \n",
    "    # Define the Disease Siamese Network model\n",
    "    siamese_network_disease = Model(inputs=[input_a, input_b], outputs=output, name='Siamese_Network_Disease')\n",
    "    \n",
    "    return siamese_network_disease\n",
    "\n",
    "# Create the Disease Siamese Network\n",
    "siamese_network_disease = create_siamese_network_shared_base(base_network)\n",
    "\n",
    "# Display the Disease Siamese Network summary\n",
    "siamese_network_disease.summary()\n",
    "\n",
    "\n",
    "# Compile the Disease Siamese Network\n",
    "siamese_network_disease.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Disease Siamese Network compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72414658-107b-4db2-9336-743b3ba33dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "\n",
    "# Define directory to save disease model\n",
    "disease_model_dir = os.path.join(features_dir, 'disease_model')\n",
    "os.makedirs(disease_model_dir, exist_ok=True)\n",
    "\n",
    "# Define the path to save the best disease model\n",
    "checkpoint_path_disease = os.path.join(disease_model_dir, 'siamese_network_disease_best.h5')\n",
    "\n",
    "# EarlyStopping callback to stop training when validation loss doesn't improve\n",
    "early_stopping_disease = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "reduce_lr_disease = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation loss\n",
    "model_checkpoint_disease = ModelCheckpoint(\n",
    "    filepath=checkpoint_path_disease,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Combine all callbacks into a list\n",
    "callbacks_disease = [early_stopping_disease, reduce_lr_disease, model_checkpoint_disease]\n",
    "\n",
    "print(\"Disease Callbacks defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c5955-afde-4fb4-857d-2492055e24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs_disease = 50\n",
    "batch_size_disease = 128  #\n",
    "\n",
    "# Train the Disease Siamese Network\n",
    "history_disease = siamese_network_disease.fit(\n",
    "    [train_disease_pair_features_1, train_disease_pair_features_2],\n",
    "    train_disease_pair_labels,\n",
    "    validation_data=([val_disease_pair_features_1, val_disease_pair_features_2], val_disease_pair_labels),\n",
    "    epochs=epochs_disease,\n",
    "    batch_size=batch_size_disease,\n",
    "    callbacks=callbacks_disease,\n",
    "    verbose=1  \n",
    ")\n",
    "\n",
    "print(\"Disease Training completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874962a6-f6de-4e9e-8b7b-69c2473126b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the final disease model\n",
    "final_disease_model_path = os.path.join(disease_model_dir, 'disease_final_model.h5')\n",
    "\n",
    "# Save the final Disease Siamese Network model\n",
    "siamese_network_disease.save(final_disease_model_path)\n",
    "\n",
    "print(f\"Disease Siamese Network model saved at: {final_disease_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83eb5b-4a62-4d07-80f1-047f488436a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_prototypes(features, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Creates prototype feature vectors for each class.\n",
    "\n",
    "    Args:\n",
    "        features (np.array): Array of feature vectors.\n",
    "        labels (np.array): Array of encoded class labels.\n",
    "        num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        prototypes (dict): Dictionary mapping class labels to prototype vectors.\n",
    "    \"\"\"\n",
    "    prototypes = {}\n",
    "    for cls in range(num_classes):\n",
    "        class_features = features[labels == cls]\n",
    "        if len(class_features) == 0:\n",
    "            print(f\"Warning: No samples for class {cls}\")\n",
    "            continue\n",
    "        prototype = np.mean(class_features, axis=0)\n",
    "        prototypes[cls] = prototype\n",
    "    return prototypes\n",
    "\n",
    "# Number of disease classes\n",
    "num_disease_classes = len(disease_label_encoder.classes_)\n",
    "\n",
    "# Create prototypes for disease classes using training data\n",
    "disease_train_prototypes = create_class_prototypes(\n",
    "    features=train_features,\n",
    "    labels=train_disease_labels_array,\n",
    "    num_classes=num_disease_classes\n",
    ")\n",
    "\n",
    "# Display a prototype for the first disease class\n",
    "first_disease_class = disease_label_encoder.inverse_transform([0])[0]\n",
    "print(f\"Prototype for disease class '{first_disease_class}': {disease_train_prototypes[0][:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371106c2-cf1b-47db-a073-7eca9044dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def classify_image(feature, prototypes, label_encoder, top_k=5):\n",
    "    \"\"\"\n",
    "    Classifies an image feature by finding the top_k closest class prototypes based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        feature (np.array): Feature vector of the image.\n",
    "        prototypes (dict): Dictionary of class prototypes.\n",
    "        label_encoder (LabelEncoder): LabelEncoder to inverse transform class labels.\n",
    "        top_k (int): Number of top similar classes to consider.\n",
    "\n",
    "    Returns:\n",
    "        top_classes (list): List of top_k predicted class labels.\n",
    "        similarity_scores_sorted (list): List of corresponding similarity scores.\n",
    "    \"\"\"\n",
    "    similarity_scores = {}\n",
    "    for cls, prototype in prototypes.items():\n",
    "        similarity = 1 - cosine(feature, prototype)  # Cosine similarity\n",
    "        similarity_scores[cls] = similarity\n",
    "\n",
    "    # Sort classes based on similarity scores in descending order\n",
    "    sorted_classes = sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    # Extract top_k classes and their scores\n",
    "    top_classes = [label_encoder.inverse_transform([cls])[0] for cls, score in sorted_classes[:top_k]]\n",
    "    similarity_scores_sorted = [score for cls, score in sorted_classes[:top_k]]\n",
    "    \n",
    "    return top_classes, similarity_scores_sorted\n",
    "\n",
    "def evaluate_model_on_dataset(features, labels, prototypes, label_encoder, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluates the model's performance on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.array): Array of feature vectors.\n",
    "        labels (np.array): Array of true encoded class labels.\n",
    "        prototypes (dict): Dictionary of class prototypes.\n",
    "        label_encoder (LabelEncoder): LabelEncoder to inverse transform class labels.\n",
    "        top_k (int): Number of top similar classes to consider.\n",
    "\n",
    "    Returns:\n",
    "        top1_accuracy (float): Top-1 accuracy.\n",
    "        top5_accuracy (float): Top-5 accuracy.\n",
    "    \"\"\"\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = len(labels)\n",
    "    \n",
    "    for i in range(total):\n",
    "        feature = features[i]\n",
    "        true_label = label_encoder.inverse_transform([labels[i]])[0]\n",
    "        predicted_topk, _ = classify_image(feature, prototypes, label_encoder, top_k=top_k)\n",
    "        \n",
    "        if true_label == predicted_topk[0]:\n",
    "            correct_top1 += 1\n",
    "            correct_top5 += 1\n",
    "        elif true_label in predicted_topk:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    top1_accuracy = correct_top1 / total\n",
    "    top5_accuracy = correct_top5 / total\n",
    "    \n",
    "    return top1_accuracy, top5_accuracy\n",
    "\n",
    "print(\"Classification functions defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d138d7c7-2ce1-424a-801d-4b15a3277531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract disease labels as a NumPy array\n",
    "disease_train_labels_eval = train_disease_labels_array\n",
    "disease_val_labels_eval = val_disease_labels_array\n",
    "\n",
    "# Evaluate on Disease Training Data\n",
    "train_disease_top1_acc, train_disease_top5_acc = evaluate_model_on_dataset(\n",
    "    features=train_features,\n",
    "    labels=disease_train_labels_eval,\n",
    "    prototypes=disease_train_prototypes,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Disease Training Data - Top-1 Accuracy: {train_disease_top1_acc * 100:.2f}%\")\n",
    "print(f\"Disease Training Data - Top-5 Accuracy: {train_disease_top5_acc * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on Disease Validation Data\n",
    "val_disease_top1_acc, val_disease_top5_acc = evaluate_model_on_dataset(\n",
    "    features=test_seen_features,\n",
    "    labels=disease_val_labels_eval,\n",
    "    prototypes=disease_train_prototypes,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Disease Validation Data - Top-1 Accuracy: {val_disease_top1_acc * 100:.2f}%\")\n",
    "print(f\"Disease Validation Data - Top-5 Accuracy: {val_disease_top5_acc * 100:.2f}%\")\n",
    "\n",
    "# Encode the 'disease_class' labels in test_unseen_data using the same LabelEncoder\n",
    "try:\n",
    "    test_unseen_data['encoded_disease_class'] = disease_label_encoder.transform(test_unseen_data['disease_class'])\n",
    "    print(\"\\nEncoded Test Unseen Disease Labels:\")\n",
    "    print(test_unseen_data[['disease_class', 'encoded_disease_class']].head())\n",
    "    \n",
    "    # Display unique encoded classes in test_unseen_data\n",
    "    print(\"\\nUnique Encoded Classes in Test Unseen Data:\", test_unseen_data['encoded_disease_class'].unique())\n",
    "except ValueError as e:\n",
    "    print(f\"Error encoding test_unseen_data: {e}\")\n",
    "    \n",
    "    # Identify classes not in label_encoder\n",
    "    unseen_classes = set(test_unseen_data['disease_class']) - set(disease_label_encoder.classes_)\n",
    "    print(f\"Unseen classes in test_unseen_data: {unseen_classes}\")\n",
    "    \n",
    "    test_unseen_data['encoded_disease_class'] = test_unseen_data['disease_class'].apply(\n",
    "        lambda x: disease_label_encoder.transform([x])[0] if x in disease_label_encoder.classes_ else -1\n",
    "    )\n",
    "    print(\"\\nAfter handling unseen classes:\")\n",
    "    print(test_unseen_data[['disease_class', 'encoded_disease_class']].head())\n",
    "\n",
    "# Extract test unseen labels as a NumPy array\n",
    "test_unseen_labels_eval = test_unseen_data['encoded_disease_class'].values\n",
    "\n",
    "# Evaluate on Test Unseen Data\n",
    "test_unseen_top1_acc, test_unseen_top5_acc = evaluate_model_on_dataset(\n",
    "    features=test_unseen_features,\n",
    "    labels=test_unseen_labels_eval,\n",
    "    prototypes=disease_train_prototypes,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"Test Unseen Data - Top-1 Accuracy: {test_unseen_top1_acc * 100:.2f}%\")\n",
    "print(f\"Test Unseen Data - Top-5 Accuracy: {test_unseen_top5_acc * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on PlantDoc Unseen Test Data\n",
    "doc_unseen_labels_eval = doc_unseen_data['encoded_disease_class'].values\n",
    "\n",
    "doc_unseen_top1_acc, doc_unseen_top5_acc = evaluate_model_on_dataset(\n",
    "    features=doc_unseen_features,\n",
    "    labels=doc_unseen_labels_eval,\n",
    "    prototypes=disease_train_prototypes,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"PlantDoc Unseen Test Data - Top-1 Accuracy: {doc_unseen_top1_acc * 100:.2f}%\")\n",
    "print(f\"PlantDoc Unseen Test Data - Top-5 Accuracy: {doc_unseen_top5_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f7e3f-b0cf-4a35-8172-5ef65a9028b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_disease_model_path = os.path.join(disease_model_dir, 'disease_best_model.h5')\n",
    "\n",
    "# Save the best Disease Siamese Network model\n",
    "siamese_network_disease.save(best_disease_model_path)\n",
    "\n",
    "print(f\"Disease Siamese Network model saved explicitly at: {best_disease_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6502e-37ab-4cbb-9445-eb686f9c0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot the confusion matrix for validation data\n",
    "evaluate_confusion_matrix(\n",
    "    true_labels=val_disease_true_labels_decoded,\n",
    "    predicted_labels=val_disease_predicted_labels_decoded,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    title='Confusion Matrix - Disease Classification (Seen)'\n",
    ")\n",
    "# Generate and plot the confusion matrix for test unseen data\n",
    "evaluate_confusion_matrix(\n",
    "    true_labels=test_unseen_true_labels_decoded,\n",
    "    predicted_labels=test_unseen_predicted_labels_decoded,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    title='Confusion Matrix - Disease Classification (Test Unseen Data)'\n",
    ")\n",
    "# Generate and plot the confusion matrix for PlantDoc unseen test data\n",
    "evaluate_confusion_matrix(\n",
    "    true_labels=doc_unseen_true_labels_decoded,\n",
    "    predicted_labels=doc_unseen_predicted_labels_decoded,\n",
    "    label_encoder=disease_label_encoder,\n",
    "    title='Confusion Matrix - Disease Classification (PlantDoc Unseen Test Data)'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
